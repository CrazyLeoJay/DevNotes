# Ktransformers

- [å®˜æ–¹ç½‘ç«™](https://kvcache-ai.github.io/ktransformers/)
- [GitHub ç«™ç‚¹](https://github.com/kvcache-ai/ktransformers)

åšå®¢ï¼š

- [KTransformersï¼šè®©æ˜¾å¡è½»æ¾é©¾é©­æ»¡è¡€DeepSeekçš„ç§˜ç±ï¼](https://blog.csdn.net/drdairen/article/details/145612436)



[æ¸…åå¤§å­¦](https://so.csdn.net/so/search?q=æ¸…åå¤§å­¦&spm=1001.2101.3001.7020)KVCache.AIå›¢é˜Ÿè”åˆè¶‹å¢ƒç§‘æŠ€ç²¾å¿ƒæ‰“é€ çš„æ¨ç†æ¡†æ¶ã€‚æ”¯æŒåœ¨ä»…æœ‰24GBæ˜¾å­˜çš„æ¶ˆè´¹çº§æ˜¾å¡ä¸Šæµç•…è¿è¡ŒDeepSeek-R1ã€V3çš„671Bæ»¡è¡€ç‰ˆï¼ä½ æ²¡å¬é”™ï¼Œå°±æ˜¯671Bé‚£ä¸ªå¤§å®¶ä¼™ï¼



KTransformersï¼Œå‘éŸ³ä¸ºå¿«é€Ÿå˜å‹å™¨ï¼Œæ—¨åœ¨æé«˜æ‚¨çš„ğŸ¤—å…·æœ‰é«˜çº§å†…æ ¸ä¼˜åŒ–å’Œæ”¾ç½®/å¹¶è¡Œæ€§ç­–ç•¥çš„å˜å‹å™¨ç»éªŒã€‚

KTransformersæ˜¯ä¸€ä¸ªçµæ´»çš„ï¼Œä»¥Pythonä¸ºä¸­å¿ƒçš„æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯å¯æ‰©å±•æ€§ã€‚é€šè¿‡ä½¿ç”¨å•è¡Œä»£ç å®ç°å’Œæ³¨å…¥ä¼˜åŒ–çš„æ¨¡å—ï¼Œç”¨æˆ·å¯ä»¥è®¿é—®ä¸Transformerså…¼å®¹çš„ç•Œé¢ï¼Œä¸OpenAIå’ŒOllamaå…¼å®¹çš„RESTful apiï¼Œç”šè‡³æ˜¯ç®€åŒ–çš„ç±»ä¼¼ChatGPTçš„web UIã€‚

æˆ‘ä»¬å¯¹KTransformersçš„æ„¿æ™¯æ˜¯ä½œä¸ºä¸€ä¸ªçµæ´»çš„å¹³å°ï¼Œç”¨äºè¯•éªŒåˆ›æ–°çš„LLMæ¨ç†ä¼˜åŒ–ã€‚è¯·è®©æˆ‘ä»¬çŸ¥é“ï¼Œå¦‚æœä½ éœ€è¦ä»»ä½•å…¶ä»–åŠŸèƒ½ã€‚



## å‡†å¤‡å·¥ä½œ



ä¸€äº›å‡†å¤‡å·¥ä½œ:

- CUDA 12.1åŠä»¥ä¸Šï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰ï¼Œä½ å¯ä»¥ä»è¿™é‡Œå®‰è£…ã€‚

  ```
  # Adding CUDA to PATH
  export PATH=/usr/local/cuda/bin:$PATH
  export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
  export CUDA_PATH=/usr/local/cuda
  ```

  

- ä¸gccï¼Œg ++ å’Œcmce Linux-x86_64

  ```
  sudo apt-get update
  sudo apt-get install gcc g++ cmake ninja-build
  ```

  

- æˆ‘ä»¬å»ºè®®ä½¿ç”¨Condaåˆ›å»ºä¸€ä¸ªå¸¦æœ‰Python = 3.11çš„è™šæ‹Ÿç¯å¢ƒæ¥è¿è¡Œæˆ‘ä»¬çš„ç¨‹åºã€‚

  ```
  conda create --name ktransformers python=3.11
  conda activate ktransformers # you may need to run â€˜conda initâ€™ and reopen shell first
  ```

  

- ç¡®ä¿å®‰è£…äº†PyTorchï¼Œpackagingï¼Œninja

  ```
  pip install torch packaging ninja cpufeature numpy
  ```

  



## å¿«é€Ÿå¼€å§‹ 

[(github åŸæ–‡)](https://github.com/kvcache-ai/ktransformers?tab=readme-ov-file#installation)

1. ä½¿ç”¨Dockeræ˜ åƒï¼Œè¯·å‚é˜… [Docker](ktransformers\docker.md) æ–‡æ¡£

2. æ‚¨å¯ä»¥ä½¿ç”¨Pypiå®‰è£… (é€‚ç”¨äºlinux):

   ```
   pip install ktransformers --no-build-isolation
   ```

   

   å¯¹äºwindowsï¼Œæˆ‘ä»¬åœ¨ktransformers-0.2.0 + cu125torch24avx2-cp312-cp312-win_amd64.whlä¸Šå‡†å¤‡ä¸€ä¸ªé¢„ç¼–è¯‘çš„whlåŒ…ï¼Œå®ƒéœ€è¦cuda-12.5ï¼Œtorch-2.4ï¼Œpython-3.11ï¼Œæ›´å¤šçš„é¢„ç¼–è¯‘åŒ…æ­£åœ¨ç”Ÿæˆã€‚

3. æˆ–è€…ä½ å¯ä»¥ä¸‹è½½æºä»£ç å¹¶ç¼–è¯‘:

   - initæºä»£ç 

     ```
     git clone https://github.com/kvcache-ai/ktransformers.git
     cd ktransformers
     git submodule init
     git submodule update
     ```

   - [å¯é€‰] å¦‚æœè¦ä¸ç½‘ç«™ä¸€èµ·è¿è¡Œï¼Œè¯·åœ¨æ‰§è¡Œbash install.shä¹‹å‰ç¼–è¯‘ç½‘ç«™

   - ç¼–è¯‘å’Œå®‰è£… (é€‚ç”¨äºLinux)

     ```
     bash install.sh
     ```

   - ç¼–è¯‘å’Œå®‰è£… (é€‚ç”¨äºWindows)

     ```
     install.bat
     ```

     

4. å¦‚æœæ‚¨æ˜¯å¼€å‘äººå‘˜ï¼Œåˆ™å¯ä»¥ä½¿ç”¨makefileæ¥ç¼–è¯‘å’Œæ ¼å¼åŒ–ä»£ç ã€‚makefileçš„è¯¦ç»†ç”¨æ³•åœ¨è¿™é‡Œ



## æœ¬åœ°èŠå¤©



æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡Œæœ¬åœ°èŠå¤©Pythonè„šæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œå®ƒè¿›è¡Œæµ‹è¯•ã€‚

> è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æµ‹è¯•å·¥å…·ï¼Œåªæ”¯æŒä¸€è½®èŠå¤©ï¼Œæ²¡æœ‰ä»»ä½•å…³äºæœ€åè¾“å…¥çš„è®°å¿†ï¼Œå¦‚æœä½ æƒ³å°è¯•æ¨¡å‹çš„å…¨éƒ¨èƒ½åŠ›ï¼Œä½ å¯ä»¥å»RESTful APIå’ŒWeb UIã€‚æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨DeepSeek-V2-Lite-Chat-GGUFæ¨¡å‹ä½œä¸ºç¤ºä¾‹ã€‚ä½†æ˜¯æˆ‘ä»¬ä¹Ÿæ”¯æŒå…¶ä»–æ¨¡å‹ï¼Œæ‚¨å¯ä»¥å°†å…¶æ›¿æ¢ä¸ºè¦æµ‹è¯•çš„ä»»ä½•å…¶ä»–æ¨¡å‹ã€‚

**è¿è¡Œç¤ºä¾‹**

```sh
# Begin from root of your cloned repo!
# Begin from root of your cloned repo!!
# Begin from root of your cloned repo!!! 

# Download mzwing/DeepSeek-V2-Lite-Chat-GGUF from huggingface
mkdir DeepSeek-V2-Lite-Chat-GGUF
cd DeepSeek-V2-Lite-Chat-GGUF

wget https://huggingface.co/mzwing/DeepSeek-V2-Lite-Chat-GGUF/resolve/main/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf

cd .. # Move to repo's root dir

# Start local chat
python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

# If you see â€œOSError: We couldn't connect to 'https://huggingface.co' to load this fileâ€, tryï¼š
# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite
# python  ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF
```



å®ƒå…·æœ‰ä»¥ä¸‹å‚æ•°:

- -- model_path (å¿…é€‰): æ¨¡å‹åç§° (å¦‚ â€œdeepseek-ai/DeepSeek-V2-Lite-Chatâ€ï¼Œä¼šè‡ªåŠ¨ä»æ‹¥æŠ±é¢ä¸‹è½½é…ç½®)ã€‚æˆ–è€…ï¼Œå¦‚æœä½ å·²ç»æœ‰äº†æœ¬åœ°æ–‡ä»¶ï¼Œä½ å¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥è·¯å¾„æ¥åˆå§‹åŒ–æ¨¡å‹ã€‚

  > æ³¨:ã€‚ç›®å½•ä¸­ä¸éœ€è¦safetensorsæ–‡ä»¶ã€‚æˆ‘ä»¬åªéœ€è¦é…ç½®æ–‡ä»¶æ¥æ„å»ºæ¨¡å‹å’Œtokenizerã€‚

- -- gguf_path (å¿…éœ€): åŒ…å«GGUFæ–‡ä»¶çš„ç›®å½•çš„è·¯å¾„ï¼Œå¯ä»¥ä»æ‹¥æŠ±é¢ä¸‹è½½ã€‚è¯·æ³¨æ„ï¼Œè¯¥ç›®å½•åº”ä»…åŒ…å«å½“å‰æ¨¡å‹çš„GGUFï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦ä¸ºæ¯ä¸ªæ¨¡å‹ä¸€ä¸ªå•ç‹¬çš„ç›®å½•ã€‚

- -- optimize_rule_path (Qwen2Moeå’ŒDeepSeek-V2é™¤å¤–): åŒ…å«ä¼˜åŒ–è§„åˆ™çš„YAMLæ–‡ä»¶çš„è·¯å¾„ã€‚åœ¨ktransformers/optimize/optimize_rulesç›®å½•ä¸­é¢„å…ˆç¼–å†™äº†ä¸¤ä¸ªè§„åˆ™æ–‡ä»¶ï¼Œç”¨äºä¼˜åŒ–DeepSeek-V2å’ŒQwen2-57B-A14ä¸¤ä¸ªSOTA MoEæ¨¡å‹ã€‚

- -- max_new_tokens: Int (é»˜è®¤å€¼ = 1000)ã€‚è¦ç”Ÿæˆçš„æ–°ä»¤ç‰Œçš„æœ€å¤§æ•°é‡ã€‚

- -- cpu_infer: Int (é»˜è®¤å€¼ = 10)ã€‚ç”¨äºæ¨ç†çš„cpuæ•°é‡ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåº”è®¾ç½®ä¸º (å†…æ ¸æ€»æ•°-2)ã€‚



è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨DeepSeekå’ŒQWenæ—¶ï¼Œæ‚¨éœ€è¦éµå®ˆå…¶ç›¸åº”çš„æ¨¡å‹è®¸å¯è¯ã€‚



## RESTful APIå’ŒWeb UI

å¼€å§‹æ²¡æœ‰ç½‘ç«™:

```
ktransformers --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path /path/to/DeepSeek-V2-Lite-Chat-GGUF --port 10002
```

ä»ç½‘ç«™å¼€å§‹:

```
ktransformers --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path /path/to/DeepSeek-V2-Lite-Chat-GGUF  --port 10002 --web True
```



æˆ–è€…ä½ æƒ³ç”¨å˜å‹å™¨å¯åŠ¨æœåŠ¡å™¨ï¼Œmodel_pathåº”è¯¥åŒ…æ‹¬safetensors

```
ktransformers --type transformers --model_path /mnt/data/model/Qwen2-0.5B-Instruct --port 10002 --web True
```

ä½¿ç”¨urlè®¿é—®ç½‘ç«™http:// localhost:10002/web/index.html #/chat:![Web UI](ktransformers.assets/615dca9b-a08c-4183-bbd3-ad1362680faf)

å…³äºRESTful APIæœåŠ¡å™¨çš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚æ‚¨è¿˜å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ä¸Tabbyé›†æˆçš„ç¤ºä¾‹ã€‚

## ğŸ“ƒç®€çŸ­çš„æ³¨å°„æ•™ç¨‹

KTransformersçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ã€åŸºäºæ¨¡æ¿çš„æ³¨å…¥æ¡†æ¶ã€‚è¿™ä½¿ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾åœ°ç”¨ä¼˜åŒ–çš„å˜ä½“æ›¿æ¢åŸå§‹çš„torchæ¨¡å—ã€‚å®ƒè¿˜ç®€åŒ–äº†ç»„åˆå¤šä¸ªä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œå…è®¸æ¢ç´¢å®ƒä»¬çš„ååŒæ•ˆåº”ã€‚

![Inject-Struction](ktransformers.assets/6b4c1e54-9f6d-45c5-a3fc-8fa45e7d257e)

é‰´äºvLLMå·²ç»ä½œä¸ºå¤§è§„æ¨¡éƒ¨ç½²ä¼˜åŒ–çš„ä¸€ä¸ªå¾ˆå¥½çš„æ¡†æ¶ï¼ŒKTransformersç‰¹åˆ«å…³æ³¨å—æœ‰é™èµ„æºçº¦æŸçš„æœ¬åœ°éƒ¨ç½²ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨å¼‚æ„è®¡ç®—æœºä¼šï¼Œä¾‹å¦‚é‡åŒ–æ¨¡å‹çš„GPU/CPUå¸è½½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åˆ†åˆ«ä¸ºCPUå’ŒGPUæ”¯æŒé«˜æ•ˆçš„Llamafileå’ŒMarlinå†…æ ¸ã€‚æ›´å¤šç»†èŠ‚å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ã€‚



**ä½¿ç”¨ç¤ºä¾‹**

è¦ä½¿ç”¨æä¾›çš„å†…æ ¸ï¼Œç”¨æˆ·åªéœ€è¦åˆ›å»ºä¸€ä¸ªåŸºäºYAMLçš„æ³¨å…¥æ¨¡æ¿ï¼Œå¹¶åœ¨ä½¿ç”¨Transformersæ¨¡å‹ä¹‹å‰æ·»åŠ å¯¹ â€œoptimize_and_load_ggufâ€ çš„è°ƒç”¨ã€‚

```
with torch.device("meta"):
    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)
...
generated = prefill_and_generate(model, tokenizer, input_tensor.cuda(), max_new_tokens=1000)
```



åœ¨è¯¥ç¤ºä¾‹ä¸­ï¼Œé¦–å…ˆåœ¨å…ƒè®¾å¤‡ä¸Šåˆå§‹åŒ–è‡ªåŠ¨æ¨¡å‹ä»¥é¿å…å ç”¨ä»»ä½•å­˜å‚¨å™¨èµ„æºã€‚ç„¶åï¼Œoptimize_and_load_gguféå†æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—ï¼ŒåŒ¹é…YAMLè§„åˆ™æ–‡ä»¶ä¸­æŒ‡å®šçš„è§„åˆ™ï¼Œå¹¶å°†å®ƒä»¬æ›¿æ¢ä¸ºæŒ‡å®šçš„é«˜çº§æ¨¡å—ã€‚

æ³¨å…¥åï¼ŒåŸå§‹çš„generateæ¥å£å¯ç”¨ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æä¾›äº†å…¼å®¹çš„prefill_and_generateæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿åƒCUDAGraphè¿™æ ·çš„è¿›ä¸€æ­¥ä¼˜åŒ–èƒ½å¤Ÿæé«˜ç”Ÿæˆé€Ÿåº¦ã€‚



## å¦‚ä½•è‡ªå®šä¹‰æ‚¨çš„æ¨¡å‹

è¿™é‡Œç»™å‡ºäº†ä½¿ç”¨DeepSeek-V2ä½œä¸ºç¤ºä¾‹çš„æ³¨å…¥å’Œå¤šGPUçš„è¯¦ç»†æ•™ç¨‹ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªYAMLæ¨¡æ¿çš„ç¤ºä¾‹ï¼Œç”¨äºç”¨é«˜çº§4ä½é‡åŒ–å†…æ ¸Marlinæ›¿æ¢æ‰€æœ‰åŸå§‹çº¿æ€§æ¨¡å—ã€‚

```
- match:
    name: "^model\\.layers\\..*$"  # regular expression 
    class: torch.nn.Linear  # only match modules matching name and class simultaneously
  replace:
    class: ktransformers.operators.linear.KTransformerLinear  # optimized Kernel on quantized data types
    device: "cpu"   # which devices to load this module when initializing
    kwargs:
      generate_device: "cuda"
      generate_linear_type: "QuantizedLinearMarlin"
```



YAMLæ–‡ä»¶ä¸­çš„æ¯ä¸ªè§„åˆ™éƒ½æœ‰ä¸¤ä¸ªéƒ¨åˆ†: åŒ¹é…å’Œæ›¿æ¢ã€‚åŒ¹é…éƒ¨åˆ†æŒ‡å®šåº”è¯¥æ›¿æ¢å“ªä¸ªæ¨¡å—ï¼Œæ›¿æ¢éƒ¨åˆ†æŒ‡å®šè¦ä¸åˆå§‹åŒ–å…³é”®å­—ä¸€èµ·æ³¨å…¥åˆ°æ¨¡å‹ä¸­çš„æ¨¡å—ã€‚

æ‚¨å¯ä»¥åœ¨ktransformers/optimize/optimize_rulesç›®å½•ä¸­æ‰¾åˆ°ç”¨äºä¼˜åŒ–DeepSeek-V2å’ŒQwen2-57B-A14 (ä¸¤ä¸ªSOTA MoEæ¨¡å‹) çš„ç¤ºä¾‹è§„åˆ™æ¨¡æ¿ã€‚è¿™äº›æ¨¡æ¿ç”¨äºæ”¯æŒlocal_chat.pyæ¼”ç¤ºã€‚

å¦‚æœæ‚¨å¯¹æˆ‘ä»¬çš„è®¾è®¡åŸç†å’Œæ³¨å…¥æ¡†æ¶çš„å®ç°æ„Ÿå…´è¶£ï¼Œè¯·å‚é˜…è®¾è®¡æ–‡æ¡£ [design document](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/deepseek-v2-injection.md).ã€‚